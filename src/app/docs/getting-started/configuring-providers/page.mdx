---
title: "Configuring LLM Providers"
pageOrder: 5
description: "Learn how to configure various LLM providers like OpenAI, Azure OpenAI, Anthropic, and Ollama for kagent."
---

export const metadata = {
  title: "Configuring LLM providers in kagent",
  description: "Learn how to configure various LLM providers like OpenAI, Azure OpenAI, Anthropic, and Ollama for kagent.",
  author: "kagent.dev"
};

# Configuring LLM providers

OpenAI gpt-4o is configured as a default LLM provider for kagent (the `default-model-config` in the kagent namespace). The ModelConfig resource is used for configuring models.

The following providers are currently supported:

- OpenAI
- Azure OpenAI
- Anthropic
- Ollama

## Configuring Anthropic

1. Create a Kubernetes Secret that stores the API key, replace `<your_api_key>` with an actual API key:

```shell
export ANTHROPIC_API_KEY=<your_api_key>
kubectl create secret generic kagent-anthropic -n kagent --from-literal ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY
```

2. Create a ModelConfig resource that references the secret and key name, and specify the Anthropic model you want to use:

```yaml
apiVersion: kagent.dev/v1alpha1
kind: ModelConfig
metadata:
  name: claude-model-config
  namespace: kagent
spec:
  apiKeySecretRef: kagent-anthropic
  apiKeySecretKey: ANTHROPIC_API_KEY
  model: claude-3-sonnet-20240229
  provider: Anthropic
  anthropic: {}
```

3. Apply the above resource to the cluster.

Once the resource is applied, you can select the model from the Model dropdown in the UI when creating or updating agents.

![Model selection](/images/modelselection.png "kagent model selection dropdown")


## Configuring Azure OpenAI

1. Create a Kubernetes Secret that stores the API key, replace `<your_api_key>` with an actual API key:

```shell
export AZURE_OPENAI_API_KEY=<your_api_key>
kubectl create secret generic kagent-azureopenai -n kagent --from-literal AZURE_OPENAI_API_KEY=$AZURE_OPENAI_API_KEY
```

2. Create a ModelConfig resource that references the secret and key name, and specify the additional information that's required for the Azure OpenAI - that's the deployment name, version and the Azure AD token. You can get these values from Azure.

```yaml
apiVersion: kagent.dev/v1alpha1
kind: ModelConfig
metadata:
  name: azureopenai-model-config
  namespace: kagent
spec:
  apiKeySecretRef: kagent-azureopenai
  apiKeySecretKey: AZURE_OPENAI_API_KEY
  model: gpt-4o-mini
  provider: AzureOpenAI
  azureOpenAI:
    azureEndpoint: "https://{yourendpointname}.openai.azure.com/"
    apiVersion: "2025-03-01-preview"
    azureDeployment: "gpt-4o-mini"
    azureAdToken: <azure_ad_token_value>
```

3. Apply the above resource to the cluster.

Once the resource is applied, you can select the model from the Model dropdown in the UI when creating or updating agents.

## Configuring Ollama

[Ollama](https://ollama.com) allows you to run LLMs locally on your computer or in a Kubernetes cluster. Configuring Ollama in kagent follows the same pattern as for other providers.

Let's give an example of how to run Ollama on a Kubernetes cluster:

1. Create a namespace for Ollama deployment and service:

```shell
kubectl create ns ollama
```

2. Create the deployment and service:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: ollama
spec:
  selector:
    matchLabels:
      name: ollama
  template:
    metadata:
      labels:
        name: ollama
    spec:
      containers:
      - name: ollama
        image: ollama/ollama:latest
        ports:
        - name: http
          containerPort: 11434
          protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: ollama
spec:
  type: ClusterIP
  selector:
    name: ollama
  ports:
  - port: 80
    name: http
    targetPort: http
    protocol: TCP
```

You can run `kubectl get pod -n ollama` and wait until the pod has started.

Once the pod has started, you can port-forward to the Ollama service and use `ollama run [model-name]` to download/run the model. You can download Ollama binary [here](https://ollama.com/download/).

>As kagent relies on calling tools, sure you're using a model that allows function calling.

Let's assume we've downloaded the `llama3` model, you can then use the following ModelConfig to configure the model:

```yaml
apiVersion: kagent.dev/v1alpha1
kind: ModelConfig
metadata:
  name: llama3-model-config
  namespace: kagent
spec:
  apiKeySecretKey: OPENAI_API_KEY
  apiKeySecretRef: kagent-openai
  model: llama3
  provider: Ollama
  ollama:
    host: http://ollama.ollama.svc.cluster.local
```

## Configuring Custom Models

Often you may want to use a custom model with an existing provider, for example if you're using a self-hosted model exposed via the OpenAI API.
Or if you're using a proxy to the OpenAI API.

We require certain information about any model to be able to use it. We ship with a default set of models, but you can also add custom models.

The example below will add an API key, but given that this is a custom provider, the key is optional.

1. Create a Kubernetes Secret that stores the API key, replace `<your_api_key>` with an actual API key:

```shell
export OPENAI_API_KEY=<your_api_key>
kubectl create secret generic kagent-openai -n kagent --from-literal OPENAI_API_KEY=$OPENAI_API_KEY
```

2. Create a ModelConfig resource that references the secret and key name, and specify the model you want to use:

```yaml
apiVersion: kagent.dev/v1alpha1
kind: ModelConfig
metadata:
  name: custom-openai-model-config
  namespace: kagent
spec:
  apiKeySecretRef: kagent-openai
  apiKeySecretKey: OPENAI_API_KEY
  model: <insert-model-name> # This should match the model name passed to the provider.
  provider: OpenAI
  openai:
    baseURL: https://your-custom-openai-provider.com
  modelInfo:
    functionCalling: true
    family: unknown
```

If you don't need an API key you can elide the `apiKeySecretRef` and `apiKeySecretKey` fields.

3. Apply the above resource to the cluster.

Once the resource is applied, you can select the model from the Model dropdown in the UI when creating or updating agents.
